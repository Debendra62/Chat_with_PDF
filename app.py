# Importing libraries
import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
from io import BytesIO

from langchain_google_genai import GoogleGenerativeAIEmbeddings
import google.generativeai as genai

from langchain.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

load_dotenv()

genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

def get_pdf_text(pdf_documents):
    """
    Extracts text from a list of PDF documents.

    Args:
        pdf_documents (list): List of PDF files uploaded by the user.r

    Returns:
        str: Combined text extracted from all the PDF files.
    """
    text = ""
    for pdf in pdf_documents:
        pdf_reader = PdfReader(BytesIO(pdf.read()))
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

def get_text_chunks(text):
    """
    Splits the text into smaller chunks for processing.

    Args:
        text (str): The text to be split into chunks.

    Returns:
        list: List of text chunks.
    """
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
    chunks = text_splitter.split_text(text)

    return chunks

def get_vector_store(text_chunks):
    """
    Creates a vector store from text chunks and saves it locally.

    Args:
        text_chunks (list): List of text chunks to be embedded and stored.
    """
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
    vector_store.save_local("faiss_index")

def get_conversational_chain():
    """
    Initializes and returns a conversational chain for question answering.

    Returns:
        Chain: A LangChain QA chain for conversational interactions.
    """
    prompt_template = """
    Answer the question as detailed as possible from the provided context, make sure to provide all the details,
    if the answer is not in the provided context just say, "The answer is not available in the context", don't provide the wrong answer. If the user asks to call them simply route them to the form.\n\n
    Context:\n {context}?\n
    Question:\n {question}\n 

    Answer:
    """

    model = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.3)
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)

    return chain

def user_input(user_question):
    """
    Handles user input, performs similarity search, and generates a response.

    Args:
        user_question (str): The question asked by the user.

    Returns:
        str: The response generated by the conversational chain.
    """
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

    db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    docs = db.similarity_search(user_question)
    
    print("Documents found for the query:")
    for doc in docs:
        print(doc.page_content)

    chain = get_conversational_chain()

    response = chain(
        {
            "input_documents": docs, "question": user_question
        },
        return_only_outputs=True
    )

    print("Generated response:")
    print(response["output_text"])

    if "call me" in user_question.lower():
        with st.form(key='user_info_form'):
            name = st.text_input("Name")
            phone = st.text_input("Phone Number")
            email = st.text_input("Email")
            submit_button = st.form_submit_button(label='Submit')

            if submit_button:
                st.write(f"Thank you, {name}. We will call you at {phone} or email you at {email} soon.")
                return f"Thank you, {name}. We will call you at {phone} or email you at {email} soon."
    
    return response["output_text"]

def main():
    """
    Main function to run the Streamlit application.
    """
    st.set_page_config(page_title="Chat with PDF", layout="wide")
    st.header("Query multiple PDFs")

    if "conversation" not in st.session_state:
        st.session_state.conversation = []

    with st.sidebar:
        st.title("Menu:")
        pdf_docs = st.file_uploader("Upload the PDF Files and Submit", type=["pdf"], accept_multiple_files=True)
        if st.button("Submit and Process"):
            with st.spinner("Processing..."):
                raw_text = get_pdf_text(pdf_docs)
                text_chunks = get_text_chunks(raw_text)
                get_vector_store(text_chunks)
                st.success("Done!")

    user_question = st.text_input("Ask a Question from the PDF Files")

    if user_question:
        response = user_input(user_question)
        st.session_state.conversation.append((user_question, response))

    if st.session_state.conversation:
        for i, (question, response) in enumerate(st.session_state.conversation):
            st.markdown(f"**You:** {question}")
            st.markdown(f"**Answer:** {response}")
            if "call me" in question.lower() and i == len(st.session_state.conversation) - 1:
                break

if __name__ == "__main__":
    main()